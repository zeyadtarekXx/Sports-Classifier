# -*- coding: utf-8 -*-
"""Sports-Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sjMe5qAjPXyFvR8dDDYQ8wKf7Lr2SsXY

## Import and functions
"""

import cv2
import numpy as np
import os
from random import shuffle
import tensorflow as tf
import matplotlib.pyplot as plt
!pip install tflearn
import tflearn
from tflearn.layers.conv import conv_2d, max_pool_2d
from tflearn.layers.core import input_data, dropout, fully_connected
from tflearn.layers.estimator import regression
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from google.colab import drive
drive.mount('/content/drive')

IMG_SIZE = 224
model_name = "sports_classification_cnn"
LR = 0.001
train_dir = "/content/drive/MyDrive/NN Dataset.zip (Unzipped Files)/Train"
test_dir = "/content/drive/MyDrive/NN Dataset.zip (Unzipped Files)/Test"

def create_label(image_name):
    label = image_name.split("_")[0]
    if label == "Basketball":
        return np.array([1,0,0,0,0,0])
    elif label == "Football":
        return np.array([0,1,0,0,0,0])
    elif label == "Rowing":
        return np.array([0,0,1,0,0,0])
    elif label == "Swimming":
        return np.array([0,0,0,1,0,0])
    elif label == "Tennis":
        return np.array([0,0,0,0,1,0])
    elif label == "Yoga":
        return np.array([0,0,0,0,0,1])
    else:
        return np.array([0,0,0,0,0,0])

def create_train_data():
    train_data = []
    train_imgs = os.listdir(train_dir)
    for img in train_imgs:
        path = os.path.join(train_dir, img)
        img_data = cv2.imread(path)
        img_data = cv2.resize(img_data, (IMG_SIZE, IMG_SIZE))
        train_data.append([np.array(img_data),create_label(img)])
    shuffle(train_data)
    np.save('train_data.npy', train_data)
    return train_data

def create_test_data():
    test_data = []
    test_imgs = os.listdir(test_dir)
    for img in test_imgs:
        path = os.path.join(test_dir, img)
        img_data = cv2.imread(path)
        img_data = cv2.resize(img_data, (IMG_SIZE, IMG_SIZE))
        test_data.append([np.array(img_data),create_label(img)])

    np.save('test_data.npy', test_data)
    return test_data



"""## Algorithm CNN"""

if os.path.exists("train_data.npy"):
    train_data=np.load("train_data.npy", allow_pickle=True)
else:
    train_data = create_train_data()


    
if os.path.exists("test_data.npy"):
    test_data=np.load("test_data.npy", allow_pickle=True)
else:
    test_data = create_test_data()

train = train_data
test = test_data
X_train = np.array([i[0] for i in train]).reshape(-1, IMG_SIZE, IMG_SIZE, 3)
y_train = np.array([i[1] for i in train])
print(X_train.shape)
print(y_train.shape)
y_train = y_train.reshape(-1, 6)
X_test = np.array([i[0] for i in test]).reshape(-1, IMG_SIZE, IMG_SIZE, 3)
y_test = np.array([i[1] for i in test])
print(X_test.shape)
print(y_test.shape)
tf.compat.v1.reset_default_graph()
conv_input = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 3], name='input')
conv1 = conv_2d(conv_input, 32, 5, activation='relu')
pool1 = max_pool_2d(conv1, 5)

conv2 = conv_2d(pool1, 64, 5, activation='relu')
pool2 = max_pool_2d(conv2, 5)

conv3 = conv_2d(pool2, 128, 5, activation='relu')
pool3 = max_pool_2d(conv3, 5)

conv4 = conv_2d(pool3, 64, 5, activation='relu')
pool4 = max_pool_2d(conv4, 5)

conv5 = conv_2d(pool4, 32, 5, activation='relu')
pool5 = max_pool_2d(conv5, 5)
fully_layer = fully_connected(pool5, 1024, activation='relu')
fully_layer = dropout(fully_layer, 0.5)

cnn_layers = fully_connected(fully_layer, 6, activation='softmax')

cnn_layers = regression(cnn_layers, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')
model = tflearn.DNN(cnn_layers, tensorboard_dir='log', tensorboard_verbose=3)
print (X_train.shape)

if (os.path.exists('model.tfl.meta')):
    model.load('./model.tfl')
else:
    model.fit( X_train, y_train, n_epoch=10,
    validation_set=(X_test,y_test),
    snapshot_step=500, show_metric=True, run_id='cnn1')
    model.save('model.tfl')
    model.p
plt.show()

"""## Algorithm Inception v2.0

"""



conv_input = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 3], name='input')
  conv1 = conv_2d(conv_input, 32, 1, activation='relu')

  conv2 = conv_2d(conv_input, 32, 1, activation='relu')
  conv2 = conv_2d(conv2, 32, 3, activation='relu')

  conv3 = conv_2d(conv_input, 32, 1, activation='relu')
  conv3 = conv_2d(conv3, 48, 3, activation='relu')
  conv3 = conv_2d(conv3, 64, 3, activation='relu')

  pool1 = max_pool_2d(conv_input, 3)
  conv4 = conv_2d(pool1, 64, 1, activation='relu')
  full_layer = tf.concat(axis=4, values=[conv1, conv2, conv3, conv4])
  fully_layer = dropout(fully_layer, 0.5)

cnn_layers = fully_connected(fully_layer, 6, activation='softmax')
cnn_layers = regression(cnn_layers, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')
model = tflearn.DNN(cnn_layers, tensorboard_dir='log', tensorboard_verbose=3)
print (X_train.shape)

if (os.path.exists('model.tfl.meta')):
    model.load('./model.tfl')
else:
    model.fit( X_train, y_train, n_epoch=10,
    validation_set=(X_test,y_test),
    snapshot_step=500, show_metric=True, run_id='cnn1')
    model.save('model.tfl')

plt.show()

"""##VGG19"""

import random
import cv2
from keras.datasets import cifar10
from keras.utils import to_categorical
from keras.models import Model
from keras.layers import Dense, GlobalAveragePooling2D, Dropout
from keras.optimizers import SGD
from keras.applications.vgg19 import VGG19
import numpy as np
from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot
# vgg_model = VGG19(weights='imagenet', include_top=False)
# SVG(model_to_dot(vgg_model).create(prog='dot', format='svg'))

from sklearn.model_selection import train_test_split
IMG_SIZE = 224
train_data = create_train_data()
test_data = create_test_data()
train = train_data
test = test_data
X_train = np.array([i[0] for i in train]).reshape(-1, IMG_SIZE, IMG_SIZE, 3)
y_train = np.array([i[1] for i in train])

y_train = y_train.reshape(-1, 6)
X_test = np.array([i[0] for i in test]).reshape(-1, IMG_SIZE, IMG_SIZE, 3)
y_test = np.array([i[1] for i in test])

train_images, validation_images, train_labels, validation_labels = train_test_split(X_train, y_train,
    test_size=0.1, random_state= 8)

def resize_data(data):
    data_upscaled = np.zeros((data.shape[0], 224, 224, 3))
    for i, img in enumerate(data):
        large_img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_CUBIC)
        data_upscaled[i] = large_img

    return data_upscaled

x_train_resized = resize_data(X_train)
x_test_resized = resize_data(X_test)

# make explained variable hot-encoded
y_train_hot_encoded = to_categorical(y_train)
y_test_hot_encoded = to_categorical(y_test)

"""##VGG16 ALGORITHM RAW"""

from keras.applications import VGG16
from keras.layers import Flatten, Dense, BatchNormalization, Activation,Dropout

vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze four convolution blocks
for layer in vgg_model.layers[:15]:
    layer.trainable = False
# Make sure you have frozen the correct layers
for i, layer in enumerate(vgg_model.layers):
    print(i, layer.name, layer.trainable)

x = vgg_model.output
x = Flatten()(x) # Flatten dimensions to for use in FC layers
x = Dense(512, activation='relu')(x)
x = Dropout(0.5)(x) # Dropout layer to reduce overfitting
x = Dense(256, activation='relu')(x)
x = Dense(6, activation='softmax')(x) # Softmax for multiclass
transfer_model = Model(inputs=vgg_model.input, outputs=x)

from keras.callbacks import ModelCheckpoint
from keras.callbacks import ReduceLROnPlateau
lr_reduce = ReduceLROnPlateau(monitor='loss', factor=0.6, patience=8, verbose=1, mode='max', min_lr=5e-5)
checkpoint = ModelCheckpoint('vgg16_finetune.h15', monitor= 'val_accuracy', mode= 'max', save_best_only = True, verbose= 1)

from keras import layers, models, Model, optimizers
learning_rate= 5e-5
transfer_model.compile(loss="categorical_crossentropy", optimizer=optimizers.Adam(lr=learning_rate), metrics=["accuracy"])
history = transfer_model.fit(X_train, y_train, batch_size = 1, epochs=50, validation_data=(X_test,y_test), callbacks=[lr_reduce,checkpoint])

import matplotlib.pyplot as plt
def show_history(history):
    plt.plot(history.history['acc'])
    plt.plot(history.history['val_acc'])
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train_accuracy', 'test_accuracy'], loc='best')
    plt.show()

show_history(history)

# import pickle
# with open('/content/VGG16_RAW.pkl', 'wb') as fp:
#     pickle.dump(transfer_model, fp)
  
transfer_model.save('VGG16_RAW.h5')
transfer_model.save_weights('VGG16_RAW_Weights.h5')

"""##VGG ALGORITHM AUG"""

from keras.applications import VGG16
from keras.layers import Flatten, Dense, BatchNormalization, Activation,Dropout

vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

for layer in vgg_model.layers[:15]:
  layer.trainable = False
x = vgg_model.output
x = Flatten()(x) # Flatten dimensions to for use in FC layers
x = Dense(512, activation='relu')(x)
x = Dropout(0.5)(x) # Dropout layer to reduce overfitting
x = Dense(256, activation='relu')(x)
x = Dense(6, activation='softmax')(x) # Softmax for multiclass
transfer_model = Model(inputs=vgg_model.input, outputs=x)


from keras import layers, models, Model, optimizers
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
from keras.callbacks import ModelCheckpoint


lr_reduce = ReduceLROnPlateau(monitor='val_acc', factor=0.6, patience=8, verbose=1, mode='max', min_lr=5e-5)
checkpoint = ModelCheckpoint('vgg16_finetune.h15', monitor= 'val_accuracy', mode= 'max', save_best_only = True, verbose= 1)


#Augment images
train_datagen = ImageDataGenerator(zoom_range=0.2, rotation_range=30, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2)
#Fit augmentation to training images
train_generator = train_datagen.flow(X_train,y_train,batch_size=100)
#Compile model
transfer_model.compile(loss="categorical_crossentropy", optimizer='adam', metrics=["accuracy"])
#Fit model
history = transfer_model.fit_generator(train_generator, validation_data=(X_test,y_test), epochs=50, shuffle=True, callbacks=[lr_reduce],verbose=1)

transfer_model.save('model_vgg16_off.h5')
transfer_model.save_weights('model_vgg16_weights_off.h5')



import matplotlib.pyplot as plt
def show_history(history):
    plt.plot(history.history['acc'])
    plt.plot(history.history['val_acc'])
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train_accuracy', 'test_accuracy'], loc='best')
    plt.show()

show_history(history)

predd=transfer_model.predict(X_test)
label=np.argmax(predd, axis=1)
imgs_name=os.listdir(test_dir)
import pandas as pd
df = pd.DataFrame({'image_name': imgs_name,
                   'label': label})
print(df)
df.to_csv("sport_2.csv",index=False)



from keras.models import Sequential, Model, load_model
import pickle

import keras.utils as image
from keras.applications.vgg16 import VGG16, preprocess_input
import numpy as np
import os
import pandas as pd
import os
import cv2

transfer_model = load_model('/content/VGG16_RAW.h5')


from sklearn.model_selection import train_test_split
IMG_SIZE = 224

test_data = create_test_data()
test = test_data

X_test = np.array([i[0] for i in test]).reshape(-1, IMG_SIZE, IMG_SIZE, 3)

def resize_data(data):
    data_upscaled = np.zeros((data.shape[0], 224, 224, 3))
    for i, img in enumerate(data):
        large_img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_CUBIC)
        data_upscaled[i] = large_img

    return data_upscaled

test = resize_data(X_test)


predd=transfer_model.predict(test)
label=np.argmax(predd, axis=1)
imgs_name=os.listdir(test_dir)
import pandas as pd
df = pd.DataFrame({'image_name': imgs_name,
                   'label': label})
print(df)
df.to_csv("sport_2.csv",index=False)



# img = image.load_img(img_path, target_size=(224,224))
# x = image.img_to_array(img)
# x = np.expand_dims(x, axis=0)
# x = preprocess_input(x)
# pre = model.predict(x)
# mmm = np.argmax(pre , axis=1)+1
# print(mmm)