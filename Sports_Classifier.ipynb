{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT_uHJ7Xa_hk"
      },
      "source": [
        "## Import and functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-X0jar6a_hy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57960e1a-f8af-49d9-f383-c4a141f7dc0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 38.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tflearn) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from tflearn) (7.1.2)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=e7ffd652e7cd6dc1ec6140654dbcf9910d87c09ccb421e0e2317bb6aa2a7c080\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/9b/15/cb1e6b279c14ed897530d15cfd7da8e3df8a947e593f5cfe59\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from random import shuffle\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install tflearn\n",
        "import tflearn\n",
        "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
        "from tflearn.layers.core import input_data, dropout, fully_connected\n",
        "from tflearn.layers.estimator import regression\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "y2yFEyUPbW3M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5878c72d-7ed7-4cc3-d0f8-d2242016096d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elByVQ5ja_h3"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 224\n",
        "model_name = \"sports_classification_cnn\"\n",
        "LR = 0.001\n",
        "train_dir = \"/content/drive/MyDrive/NN Dataset.zip (Unzipped Files)/Train\"\n",
        "test_dir = \"/content/drive/MyDrive/NN Dataset.zip (Unzipped Files)/Test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_nGijPra_h6"
      },
      "outputs": [],
      "source": [
        "def create_label(image_name):\n",
        "    label = image_name.split(\"_\")[0]\n",
        "    if label == \"Basketball\":\n",
        "        return np.array([1,0,0,0,0,0])\n",
        "    elif label == \"Football\":\n",
        "        return np.array([0,1,0,0,0,0])\n",
        "    elif label == \"Rowing\":\n",
        "        return np.array([0,0,1,0,0,0])\n",
        "    elif label == \"Swimming\":\n",
        "        return np.array([0,0,0,1,0,0])\n",
        "    elif label == \"Tennis\":\n",
        "        return np.array([0,0,0,0,1,0])\n",
        "    elif label == \"Yoga\":\n",
        "        return np.array([0,0,0,0,0,1])\n",
        "    else:\n",
        "        return np.array([0,0,0,0,0,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jg9loEb0a_h8"
      },
      "outputs": [],
      "source": [
        "def create_train_data():\n",
        "    train_data = []\n",
        "    train_imgs = os.listdir(train_dir)\n",
        "    for img in train_imgs:\n",
        "        path = os.path.join(train_dir, img)\n",
        "        img_data = cv2.imread(path)\n",
        "        img_data = cv2.resize(img_data, (IMG_SIZE, IMG_SIZE))\n",
        "        train_data.append([np.array(img_data),create_label(img)])\n",
        "    shuffle(train_data)\n",
        "    np.save('train_data.npy', train_data)\n",
        "    return train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1X2ArCua_h-"
      },
      "outputs": [],
      "source": [
        "def create_test_data():\n",
        "    test_data = []\n",
        "    test_imgs = os.listdir(test_dir)\n",
        "    for img in test_imgs:\n",
        "        path = os.path.join(test_dir, img)\n",
        "        img_data = cv2.imread(path)\n",
        "        img_data = cv2.resize(img_data, (IMG_SIZE, IMG_SIZE))\n",
        "        test_data.append([np.array(img_data),create_label(img)])\n",
        "\n",
        "    np.save('test_data.npy', test_data)\n",
        "    return test_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ecN3TKxDbtCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7yJ9t-Ka_iA"
      },
      "source": [
        "## Algorithm CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyDL7Ctsa_iC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6f1399c-0880-441d-9fc9-a7326fbf2b4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Step: 65  | total loss: \u001b[1m\u001b[32m1.11747\u001b[0m\u001b[0m | time: 50.427s\n",
            "\u001b[2K\r| Adam | epoch: 003 | loss: 1.11747 - acc: 0.5838 -- iter: 0704/1681\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(\"train_data.npy\"):\n",
        "    train_data=np.load(\"train_data.npy\", allow_pickle=True)\n",
        "else:\n",
        "    train_data = create_train_data()\n",
        "\n",
        "\n",
        "    \n",
        "if os.path.exists(\"test_data.npy\"):\n",
        "    test_data=np.load(\"test_data.npy\", allow_pickle=True)\n",
        "else:\n",
        "    test_data = create_test_data()\n",
        "\n",
        "train = train_data\n",
        "test = test_data\n",
        "X_train = np.array([i[0] for i in train]).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
        "y_train = np.array([i[1] for i in train])\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "y_train = y_train.reshape(-1, 6)\n",
        "X_test = np.array([i[0] for i in test]).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
        "y_test = np.array([i[1] for i in test])\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n",
        "tf.compat.v1.reset_default_graph()\n",
        "conv_input = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 3], name='input')\n",
        "conv1 = conv_2d(conv_input, 32, 5, activation='relu')\n",
        "pool1 = max_pool_2d(conv1, 5)\n",
        "\n",
        "conv2 = conv_2d(pool1, 64, 5, activation='relu')\n",
        "pool2 = max_pool_2d(conv2, 5)\n",
        "\n",
        "conv3 = conv_2d(pool2, 128, 5, activation='relu')\n",
        "pool3 = max_pool_2d(conv3, 5)\n",
        "\n",
        "conv4 = conv_2d(pool3, 64, 5, activation='relu')\n",
        "pool4 = max_pool_2d(conv4, 5)\n",
        "\n",
        "conv5 = conv_2d(pool4, 32, 5, activation='relu')\n",
        "pool5 = max_pool_2d(conv5, 5)\n",
        "fully_layer = fully_connected(pool5, 1024, activation='relu')\n",
        "fully_layer = dropout(fully_layer, 0.5)\n",
        "\n",
        "cnn_layers = fully_connected(fully_layer, 6, activation='softmax')\n",
        "\n",
        "cnn_layers = regression(cnn_layers, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n",
        "model = tflearn.DNN(cnn_layers, tensorboard_dir='log', tensorboard_verbose=3)\n",
        "print (X_train.shape)\n",
        "\n",
        "if (os.path.exists('model.tfl.meta')):\n",
        "    model.load('./model.tfl')\n",
        "else:\n",
        "    model.fit( X_train, y_train, n_epoch=10,\n",
        "    validation_set=(X_test,y_test),\n",
        "    snapshot_step=500, show_metric=True, run_id='cnn1')\n",
        "    model.save('model.tfl')\n",
        "    model.p\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithm Inception v2.0\n"
      ],
      "metadata": {
        "id": "jb1xuMTaYA__"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ukmMk8tkcuEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  conv_input = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 3], name='input')\n",
        "  conv1 = conv_2d(conv_input, 32, 1, activation='relu')\n",
        "\n",
        "  conv2 = conv_2d(conv_input, 32, 1, activation='relu')\n",
        "  conv2 = conv_2d(conv2, 32, 3, activation='relu')\n",
        "\n",
        "  conv3 = conv_2d(conv_input, 32, 1, activation='relu')\n",
        "  conv3 = conv_2d(conv3, 48, 3, activation='relu')\n",
        "  conv3 = conv_2d(conv3, 64, 3, activation='relu')\n",
        "\n",
        "  pool1 = max_pool_2d(conv_input, 3)\n",
        "  conv4 = conv_2d(pool1, 64, 1, activation='relu')\n",
        "  full_layer = tf.concat(axis=4, values=[conv1, conv2, conv3, conv4])\n",
        "  fully_layer = dropout(fully_layer, 0.5)\n",
        "\n",
        "cnn_layers = fully_connected(fully_layer, 6, activation='softmax')\n",
        "cnn_layers = regression(cnn_layers, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n",
        "model = tflearn.DNN(cnn_layers, tensorboard_dir='log', tensorboard_verbose=3)\n",
        "print (X_train.shape)\n",
        "\n",
        "if (os.path.exists('model.tfl.meta')):\n",
        "    model.load('./model.tfl')\n",
        "else:\n",
        "    model.fit( X_train, y_train, n_epoch=10,\n",
        "    validation_set=(X_test,y_test),\n",
        "    snapshot_step=500, show_metric=True, run_id='cnn1')\n",
        "    model.save('model.tfl')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "XbuSg4pAYKwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##VGG19"
      ],
      "metadata": {
        "id": "3Mdfe2oTbc8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import cv2\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from keras.optimizers import SGD\n",
        "from keras.applications.vgg19 import VGG19\n",
        "import numpy as np\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "# vgg_model = VGG19(weights='imagenet', include_top=False)\n",
        "# SVG(model_to_dot(vgg_model).create(prog='dot', format='svg'))"
      ],
      "metadata": {
        "id": "TNySRIhSbcZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "IMG_SIZE = 224\n",
        "train_data = create_train_data()\n",
        "test_data = create_test_data()\n",
        "train = train_data\n",
        "test = test_data\n",
        "X_train = np.array([i[0] for i in train]).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
        "y_train = np.array([i[1] for i in train])\n",
        "\n",
        "y_train = y_train.reshape(-1, 6)\n",
        "X_test = np.array([i[0] for i in test]).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
        "y_test = np.array([i[1] for i in test])\n",
        "\n",
        "train_images, validation_images, train_labels, validation_labels = train_test_split(X_train, y_train,\n",
        "    test_size=0.1, random_state= 8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q_pcqHmbiKF",
        "outputId": "6b1d6423-4204-4ca5-8712-40e644a10447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = np.asanyarray(arr)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_data(data):\n",
        "    data_upscaled = np.zeros((data.shape[0], 224, 224, 3))\n",
        "    for i, img in enumerate(data):\n",
        "        large_img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_CUBIC)\n",
        "        data_upscaled[i] = large_img\n",
        "\n",
        "    return data_upscaled"
      ],
      "metadata": {
        "id": "HbhiL8ZGdNEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_resized = resize_data(X_train)\n",
        "x_test_resized = resize_data(X_test)\n",
        "\n",
        "# make explained variable hot-encoded\n",
        "y_train_hot_encoded = to_categorical(y_train)\n",
        "y_test_hot_encoded = to_categorical(y_test)\n"
      ],
      "metadata": {
        "id": "gDWTAXNedaZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##VGG16 ALGORITHM RAW"
      ],
      "metadata": {
        "id": "lpI4VXB4rItR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications import VGG16\n",
        "from keras.layers import Flatten, Dense, BatchNormalization, Activation,Dropout\n",
        "\n",
        "vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze four convolution blocks\n",
        "for layer in vgg_model.layers[:15]:\n",
        "    layer.trainable = False\n",
        "# Make sure you have frozen the correct layers\n",
        "for i, layer in enumerate(vgg_model.layers):\n",
        "    print(i, layer.name, layer.trainable)\n",
        "\n",
        "x = vgg_model.output\n",
        "x = Flatten()(x) # Flatten dimensions to for use in FC layers\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x) # Dropout layer to reduce overfitting\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dense(6, activation='softmax')(x) # Softmax for multiclass\n",
        "transfer_model = Model(inputs=vgg_model.input, outputs=x)\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "lr_reduce = ReduceLROnPlateau(monitor='loss', factor=0.6, patience=8, verbose=1, mode='max', min_lr=5e-5)\n",
        "checkpoint = ModelCheckpoint('vgg16_finetune.h15', monitor= 'val_accuracy', mode= 'max', save_best_only = True, verbose= 1)\n",
        "\n",
        "from keras import layers, models, Model, optimizers\n",
        "learning_rate= 5e-5\n",
        "transfer_model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=learning_rate), metrics=[\"accuracy\"])\n",
        "history = transfer_model.fit(X_train, y_train, batch_size = 1, epochs=50, validation_data=(X_test,y_test), callbacks=[lr_reduce,checkpoint])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "def show_history(history):\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train_accuracy', 'test_accuracy'], loc='best')\n",
        "    plt.show()\n",
        "\n",
        "show_history(history)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LOIf2EH5hrKt",
        "outputId": "78c692ee-a35d-4cf1-f8a7-ae987c2c1eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 4s 0us/step\n",
            "0 input_1 False\n",
            "1 block1_conv1 False\n",
            "2 block1_conv2 False\n",
            "3 block1_pool False\n",
            "4 block2_conv1 False\n",
            "5 block2_conv2 False\n",
            "6 block2_pool False\n",
            "7 block3_conv1 False\n",
            "8 block3_conv2 False\n",
            "9 block3_conv3 False\n",
            "10 block3_pool False\n",
            "11 block4_conv1 False\n",
            "12 block4_conv2 False\n",
            "13 block4_conv3 False\n",
            "14 block4_pool False\n",
            "15 block5_conv1 True\n",
            "16 block5_conv2 True\n",
            "17 block5_conv3 True\n",
            "18 block5_pool True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 1681 samples, validate on 688 samples\n",
            "Epoch 1/50\n",
            "1679/1681 [============================>.] - ETA: 0s - loss: 1.7779 - acc: 0.6093"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/engine/training_v1.py:2045: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n",
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 49s 24ms/sample - loss: 1.7766 - acc: 0.6092 - val_loss: 0.0000e+00 - val_acc: 0.2849 - lr: 5.0000e-05\n",
            "Epoch 2/50\n",
            "1680/1681 [============================>.] - ETA: 0s - loss: 0.5855 - acc: 0.8321"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.5852 - acc: 0.8322 - val_loss: 0.0000e+00 - val_acc: 0.0654 - lr: 5.0000e-05\n",
            "Epoch 3/50\n",
            "1679/1681 [============================>.] - ETA: 0s - loss: 0.4868 - acc: 0.8725"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 41s 24ms/sample - loss: 0.4863 - acc: 0.8727 - val_loss: 0.0000e+00 - val_acc: 0.1410 - lr: 5.0000e-05\n",
            "Epoch 4/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.3286 - acc: 0.9114"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.3286 - acc: 0.9114 - val_loss: 0.0000e+00 - val_acc: 0.1512 - lr: 5.0000e-05\n",
            "Epoch 5/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.3549 - acc: 0.9102"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.3549 - acc: 0.9102 - val_loss: 0.0000e+00 - val_acc: 0.0828 - lr: 5.0000e-05\n",
            "Epoch 6/50\n",
            "1680/1681 [============================>.] - ETA: 0s - loss: 0.2694 - acc: 0.9351"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.2702 - acc: 0.9346 - val_loss: 0.0000e+00 - val_acc: 0.0770 - lr: 5.0000e-05\n",
            "Epoch 7/50\n",
            "1680/1681 [============================>.] - ETA: 0s - loss: 0.2511 - acc: 0.9417"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 40s 24ms/sample - loss: 0.2510 - acc: 0.9417 - val_loss: 0.0000e+00 - val_acc: 0.1802 - lr: 5.0000e-05\n",
            "Epoch 8/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.2511 - acc: 0.9381"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 40s 24ms/sample - loss: 0.2511 - acc: 0.9381 - val_loss: 0.0000e+00 - val_acc: 0.0974 - lr: 5.0000e-05\n",
            "Epoch 9/50\n",
            "1680/1681 [============================>.] - ETA: 0s - loss: 0.2930 - acc: 0.9345"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 41s 25ms/sample - loss: 0.2930 - acc: 0.9346 - val_loss: 0.0000e+00 - val_acc: 0.0843 - lr: 5.0000e-05\n",
            "Epoch 10/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.2572 - acc: 0.9459"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.2572 - acc: 0.9459 - val_loss: 0.0000e+00 - val_acc: 0.1526 - lr: 5.0000e-05\n",
            "Epoch 11/50\n",
            "1679/1681 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9708"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.1353 - acc: 0.9709 - val_loss: 0.0000e+00 - val_acc: 0.1032 - lr: 5.0000e-05\n",
            "Epoch 12/50\n",
            "1680/1681 [============================>.] - ETA: 0s - loss: 0.2764 - acc: 0.9530"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 40s 24ms/sample - loss: 0.2763 - acc: 0.9530 - val_loss: 0.0000e+00 - val_acc: 0.0741 - lr: 5.0000e-05\n",
            "Epoch 13/50\n",
            "1680/1681 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9833"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 40s 24ms/sample - loss: 0.0832 - acc: 0.9833 - val_loss: 0.0000e+00 - val_acc: 0.1003 - lr: 5.0000e-05\n",
            "Epoch 14/50\n",
            "1680/1681 [============================>.] - ETA: 0s - loss: 0.3476 - acc: 0.9613"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.3474 - acc: 0.9613 - val_loss: 0.0000e+00 - val_acc: 0.1323 - lr: 5.0000e-05\n",
            "Epoch 15/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.0856 - acc: 0.9810"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.0856 - acc: 0.9810 - val_loss: 0.0000e+00 - val_acc: 0.1206 - lr: 5.0000e-05\n",
            "Epoch 16/50\n",
            "1679/1681 [============================>.] - ETA: 0s - loss: 0.2974 - acc: 0.9672"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.3371 - acc: 0.9667 - val_loss: 0.0000e+00 - val_acc: 0.1817 - lr: 5.0000e-05\n",
            "Epoch 17/50\n",
            "1680/1681 [============================>.] - ETA: 0s - loss: 0.1721 - acc: 0.9732"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.1720 - acc: 0.9732 - val_loss: 0.0000e+00 - val_acc: 0.1148 - lr: 5.0000e-05\n",
            "Epoch 18/50\n",
            "1680/1681 [============================>.] - ETA: 0s - loss: 0.5710 - acc: 0.9518"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.5707 - acc: 0.9518 - val_loss: 0.0000e+00 - val_acc: 0.1323 - lr: 5.0000e-05\n",
            "Epoch 19/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.0478 - acc: 0.9869"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.0478 - acc: 0.9869 - val_loss: 0.0000e+00 - val_acc: 0.1061 - lr: 5.0000e-05\n",
            "Epoch 20/50\n",
            "1680/1681 [============================>.] - ETA: 0s - loss: 0.0817 - acc: 0.9839"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.0817 - acc: 0.9839 - val_loss: 0.0000e+00 - val_acc: 0.0799 - lr: 5.0000e-05\n",
            "Epoch 21/50\n",
            "1679/1681 [============================>.] - ETA: 0s - loss: 0.2726 - acc: 0.9702"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 41s 24ms/sample - loss: 0.2733 - acc: 0.9697 - val_loss: 0.0000e+00 - val_acc: 0.1105 - lr: 5.0000e-05\n",
            "Epoch 22/50\n",
            "1680/1681 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9792"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 40s 24ms/sample - loss: 0.1354 - acc: 0.9792 - val_loss: 0.0000e+00 - val_acc: 0.1962 - lr: 5.0000e-05\n",
            "Epoch 23/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.2607 - acc: 0.9738"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.2607 - acc: 0.9738 - val_loss: 0.0000e+00 - val_acc: 0.1105 - lr: 5.0000e-05\n",
            "Epoch 24/50\n",
            "1680/1681 [============================>.] - ETA: 0s - loss: 0.2434 - acc: 0.9768"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.2432 - acc: 0.9768 - val_loss: 0.0000e+00 - val_acc: 0.1570 - lr: 5.0000e-05\n",
            "Epoch 25/50\n",
            "1680/1681 [============================>.] - ETA: 0s - loss: 0.3074 - acc: 0.9714"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.3072 - acc: 0.9714 - val_loss: 0.0000e+00 - val_acc: 0.0945 - lr: 5.0000e-05\n",
            "Epoch 26/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.0296 - acc: 0.9917"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.0296 - acc: 0.9917 - val_loss: 0.0000e+00 - val_acc: 0.1308 - lr: 5.0000e-05\n",
            "Epoch 27/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.1618 - acc: 0.9845"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.1618 - acc: 0.9845 - val_loss: 0.0000e+00 - val_acc: 0.1323 - lr: 5.0000e-05\n",
            "Epoch 28/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.3746 - acc: 0.9738"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.3746 - acc: 0.9738 - val_loss: 0.0000e+00 - val_acc: 0.1265 - lr: 5.0000e-05\n",
            "Epoch 29/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.2913 - acc: 0.9798"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 38s 23ms/sample - loss: 0.2913 - acc: 0.9798 - val_loss: 0.0000e+00 - val_acc: 0.1047 - lr: 5.0000e-05\n",
            "Epoch 30/50\n",
            "1680/1681 [============================>.] - ETA: 0s - loss: 0.2375 - acc: 0.9833"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.2374 - acc: 0.9833 - val_loss: 0.0000e+00 - val_acc: 0.1395 - lr: 5.0000e-05\n",
            "Epoch 31/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.1906 - acc: 0.9845"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.1906 - acc: 0.9845 - val_loss: 0.0000e+00 - val_acc: 0.1642 - lr: 5.0000e-05\n",
            "Epoch 32/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.1445 - acc: 0.9887"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 38s 23ms/sample - loss: 0.1445 - acc: 0.9887 - val_loss: 0.0000e+00 - val_acc: 0.1221 - lr: 5.0000e-05\n",
            "Epoch 33/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.1461 - acc: 0.9923"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 38s 23ms/sample - loss: 0.1461 - acc: 0.9923 - val_loss: 0.0000e+00 - val_acc: 0.1512 - lr: 5.0000e-05\n",
            "Epoch 34/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.2149 - acc: 0.9839"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 38s 23ms/sample - loss: 0.2149 - acc: 0.9839 - val_loss: 0.0000e+00 - val_acc: 0.1294 - lr: 5.0000e-05\n",
            "Epoch 35/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.2665 - acc: 0.9887"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 38s 23ms/sample - loss: 0.2665 - acc: 0.9887 - val_loss: 0.0000e+00 - val_acc: 0.0988 - lr: 5.0000e-05\n",
            "Epoch 36/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.3011 - acc: 0.9822"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.3011 - acc: 0.9822 - val_loss: 0.0000e+00 - val_acc: 0.1119 - lr: 5.0000e-05\n",
            "Epoch 37/50\n",
            "1679/1681 [============================>.] - ETA: 0s - loss: 0.1281 - acc: 0.9911"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 40s 24ms/sample - loss: 0.1280 - acc: 0.9911 - val_loss: 0.0000e+00 - val_acc: 0.1192 - lr: 5.0000e-05\n",
            "Epoch 38/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.0923 - acc: 0.9929"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.0923 - acc: 0.9929 - val_loss: 0.0000e+00 - val_acc: 0.1105 - lr: 5.0000e-05\n",
            "Epoch 39/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.2654 - acc: 0.9869"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 38s 23ms/sample - loss: 0.2654 - acc: 0.9869 - val_loss: 0.0000e+00 - val_acc: 0.1831 - lr: 5.0000e-05\n",
            "Epoch 40/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.3379 - acc: 0.9780"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 38s 23ms/sample - loss: 0.3379 - acc: 0.9780 - val_loss: 0.0000e+00 - val_acc: 0.1032 - lr: 5.0000e-05\n",
            "Epoch 41/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.2363 - acc: 0.9881"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 38s 23ms/sample - loss: 0.2363 - acc: 0.9881 - val_loss: 0.0000e+00 - val_acc: 0.1410 - lr: 5.0000e-05\n",
            "Epoch 42/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.8122 - acc: 0.9762"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 38s 23ms/sample - loss: 0.8122 - acc: 0.9762 - val_loss: 0.0000e+00 - val_acc: 0.0974 - lr: 5.0000e-05\n",
            "Epoch 43/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.1007 - acc: 0.9958"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 38s 23ms/sample - loss: 0.1007 - acc: 0.9958 - val_loss: 0.0000e+00 - val_acc: 0.1265 - lr: 5.0000e-05\n",
            "Epoch 44/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.2060 - acc: 0.9905"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.2060 - acc: 0.9905 - val_loss: 0.0000e+00 - val_acc: 0.1265 - lr: 5.0000e-05\n",
            "Epoch 45/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.2409 - acc: 0.9893"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.2409 - acc: 0.9893 - val_loss: 0.0000e+00 - val_acc: 0.1163 - lr: 5.0000e-05\n",
            "Epoch 46/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.1939 - acc: 0.9881"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.1939 - acc: 0.9881 - val_loss: 0.0000e+00 - val_acc: 0.1090 - lr: 5.0000e-05\n",
            "Epoch 47/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.4809 - acc: 0.9845"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.4809 - acc: 0.9845 - val_loss: 0.0000e+00 - val_acc: 0.1221 - lr: 5.0000e-05\n",
            "Epoch 48/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.2070 - acc: 0.9899"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.2070 - acc: 0.9899 - val_loss: 0.0000e+00 - val_acc: 0.0770 - lr: 5.0000e-05\n",
            "Epoch 49/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.3129 - acc: 0.9893"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 39s 23ms/sample - loss: 0.3129 - acc: 0.9893 - val_loss: 0.0000e+00 - val_acc: 0.1076 - lr: 5.0000e-05\n",
            "Epoch 50/50\n",
            "1681/1681 [==============================] - ETA: 0s - loss: 0.2957 - acc: 0.9881"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1681/1681 [==============================] - 38s 23ms/sample - loss: 0.2957 - acc: 0.9881 - val_loss: 0.0000e+00 - val_acc: 0.0988 - lr: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "# with open('/content/VGG16_RAW.pkl', 'wb') as fp:\n",
        "#     pickle.dump(transfer_model, fp)\n",
        "  \n",
        "transfer_model.save('VGG16_RAW.h5')\n",
        "transfer_model.save_weights('VGG16_RAW_Weights.h5')\n"
      ],
      "metadata": {
        "id": "Ns3qzSa17Jke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##VGG ALGORITHM AUG"
      ],
      "metadata": {
        "id": "_7et--DNudqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications import VGG16\n",
        "from keras.layers import Flatten, Dense, BatchNormalization, Activation,Dropout\n",
        "\n",
        "vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "for layer in vgg_model.layers[:15]:\n",
        "  layer.trainable = False\n",
        "x = vgg_model.output\n",
        "x = Flatten()(x) # Flatten dimensions to for use in FC layers\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x) # Dropout layer to reduce overfitting\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dense(6, activation='softmax')(x) # Softmax for multiclass\n",
        "transfer_model = Model(inputs=vgg_model.input, outputs=x)\n",
        "\n",
        "\n",
        "from keras import layers, models, Model, optimizers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "lr_reduce = ReduceLROnPlateau(monitor='val_acc', factor=0.6, patience=8, verbose=1, mode='max', min_lr=5e-5)\n",
        "checkpoint = ModelCheckpoint('vgg16_finetune.h15', monitor= 'val_accuracy', mode= 'max', save_best_only = True, verbose= 1)\n",
        "\n",
        "\n",
        "#Augment images\n",
        "train_datagen = ImageDataGenerator(zoom_range=0.2, rotation_range=30, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2)\n",
        "#Fit augmentation to training images\n",
        "train_generator = train_datagen.flow(X_train,y_train,batch_size=100)\n",
        "#Compile model\n",
        "transfer_model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
        "#Fit model\n",
        "history = transfer_model.fit_generator(train_generator, validation_data=(X_test,y_test), epochs=50, shuffle=True, callbacks=[lr_reduce],verbose=1)\n",
        "\n",
        "transfer_model.save('model_vgg16_off.h5')\n",
        "transfer_model.save_weights('model_vgg16_weights_off.h5')\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "def show_history(history):\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train_accuracy', 'test_accuracy'], loc='best')\n",
        "    plt.show()\n",
        "\n",
        "show_history(history)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4BhnD_F-ugqf",
        "outputId": "3687f1a4-acb3-499e-9727-cf40cdc9fa9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 3s 0us/step\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-bfa68624eab0>:34: UserWarning: `model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = transfer_model.fit_generator(train_generator, validation_data=(X_test,y_test), epochs=50, shuffle=True, callbacks=[lr_reduce],verbose=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 98.8824 - loss: 5.2473 - acc: 0.2338"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/engine/training_v1.py:2045: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r17/17 [==============================] - 43s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 5.2494 - acc: 0.2338 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "17/17 [==============================] - 18s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.8853 - acc: 0.2487 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.9472 - acc: 0.2701 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 2.0016 - acc: 0.2677 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 2.3340 - acc: 0.2713 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7734 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7344 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7398 - acc: 0.2742 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 98.8824 - loss: 1.7252 - acc: 0.2725\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7252 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7226 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 6.0000e-04\n",
            "Epoch 11/50\n",
            "17/17 [==============================] - 29s 2s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7237 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 6.0000e-04\n",
            "Epoch 12/50\n",
            "17/17 [==============================] - 26s 2s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7233 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 6.0000e-04\n",
            "Epoch 13/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7222 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 6.0000e-04\n",
            "Epoch 14/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7225 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 6.0000e-04\n",
            "Epoch 15/50\n",
            "17/17 [==============================] - 22s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7214 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 6.0000e-04\n",
            "Epoch 16/50\n",
            "17/17 [==============================] - 22s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7223 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 6.0000e-04\n",
            "Epoch 17/50\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 98.8824 - loss: 1.7221 - acc: 0.2725\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0003600000170990825.\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7222 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 6.0000e-04\n",
            "Epoch 18/50\n",
            "17/17 [==============================] - 22s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.8651 - acc: 0.2707 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 3.6000e-04\n",
            "Epoch 19/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7226 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 3.6000e-04\n",
            "Epoch 20/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7232 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 3.6000e-04\n",
            "Epoch 21/50\n",
            "17/17 [==============================] - 23s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7209 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 3.6000e-04\n",
            "Epoch 22/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7218 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 3.6000e-04\n",
            "Epoch 23/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7233 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 3.6000e-04\n",
            "Epoch 24/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7227 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 3.6000e-04\n",
            "Epoch 25/50\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 98.8824 - loss: 1.7223 - acc: 0.2725\n",
            "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.00021600000327453016.\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7223 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 3.6000e-04\n",
            "Epoch 26/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7225 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 2.1600e-04\n",
            "Epoch 27/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7222 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 2.1600e-04\n",
            "Epoch 28/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7218 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 2.1600e-04\n",
            "Epoch 29/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7231 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 2.1600e-04\n",
            "Epoch 30/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7223 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 2.1600e-04\n",
            "Epoch 31/50\n",
            "17/17 [==============================] - 24s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7229 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 2.1600e-04\n",
            "Epoch 32/50\n",
            "17/17 [==============================] - 24s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7218 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 2.1600e-04\n",
            "Epoch 33/50\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 98.8824 - loss: 1.7227 - acc: 0.2725\n",
            "Epoch 33: ReduceLROnPlateau reducing learning rate to 0.00012960000021848827.\n",
            "17/17 [==============================] - 19s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7227 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 2.1600e-04\n",
            "Epoch 34/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7219 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 1.2960e-04\n",
            "Epoch 35/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7224 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 1.2960e-04\n",
            "Epoch 36/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7215 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 1.2960e-04\n",
            "Epoch 37/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7219 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 1.2960e-04\n",
            "Epoch 38/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7238 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 1.2960e-04\n",
            "Epoch 39/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7218 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 1.2960e-04\n",
            "Epoch 40/50\n",
            "17/17 [==============================] - 20s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7212 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 1.2960e-04\n",
            "Epoch 41/50\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 98.8824 - loss: 1.7218 - acc: 0.2725\n",
            "Epoch 41: ReduceLROnPlateau reducing learning rate to 7.775999838486313e-05.\n",
            "17/17 [==============================] - 23s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7218 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 1.2960e-04\n",
            "Epoch 42/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7223 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 7.7760e-05\n",
            "Epoch 43/50\n",
            "17/17 [==============================] - 20s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7228 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 7.7760e-05\n",
            "Epoch 44/50\n",
            "17/17 [==============================] - 20s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7210 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 7.7760e-05\n",
            "Epoch 45/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7213 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 7.7760e-05\n",
            "Epoch 46/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7226 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 7.7760e-05\n",
            "Epoch 47/50\n",
            "17/17 [==============================] - 20s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7235 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 7.7760e-05\n",
            "Epoch 48/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7226 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 7.7760e-05\n",
            "Epoch 49/50\n",
            "17/17 [==============================] - ETA: 0s - batch: 8.0000 - size: 98.8824 - loss: 1.7212 - acc: 0.2725\n",
            "Epoch 49: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7212 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 7.7760e-05\n",
            "Epoch 50/50\n",
            "17/17 [==============================] - 21s 1s/step - batch: 8.0000 - size: 98.8824 - loss: 1.7226 - acc: 0.2725 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00 - lr: 5.0000e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predd=transfer_model.predict(X_test)\n",
        "label=np.argmax(predd, axis=1)\n",
        "imgs_name=os.listdir(test_dir)\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({'image_name': imgs_name,\n",
        "                   'label': label})\n",
        "print(df)\n",
        "df.to_csv(\"sport_2.csv\",index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jtmzyfl5ts4w",
        "outputId": "f89a5466-9847-44f8-f693-e64c4cfae635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    image_name  label\n",
            "0      335.jpg      0\n",
            "1      384.jpg      0\n",
            "2      596.jpg      0\n",
            "3       15.jpg      0\n",
            "4      439.jpg      0\n",
            "..         ...    ...\n",
            "683    161.jpg      5\n",
            "684    428.jpg      5\n",
            "685    172.jpg      5\n",
            "686    279.jpg      5\n",
            "687    314.jpg      5\n",
            "\n",
            "[688 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b5vUnPJ0lK1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.models import Sequential, Model, load_model\n",
        "import pickle\n",
        "\n",
        "import keras.utils as image\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "transfer_model = load_model('/content/VGG16_RAW.h5')\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "IMG_SIZE = 224\n",
        "\n",
        "test_data = create_test_data()\n",
        "test = test_data\n",
        "\n",
        "X_test = np.array([i[0] for i in test]).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
        "\n",
        "def resize_data(data):\n",
        "    data_upscaled = np.zeros((data.shape[0], 224, 224, 3))\n",
        "    for i, img in enumerate(data):\n",
        "        large_img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_CUBIC)\n",
        "        data_upscaled[i] = large_img\n",
        "\n",
        "    return data_upscaled\n",
        "\n",
        "test = resize_data(X_test)\n",
        "\n",
        "\n",
        "predd=transfer_model.predict(test)\n",
        "label=np.argmax(predd, axis=1)\n",
        "imgs_name=os.listdir(test_dir)\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({'image_name': imgs_name,\n",
        "                   'label': label})\n",
        "print(df)\n",
        "df.to_csv(\"sport_2.csv\",index=False)\n",
        "\n",
        "\n",
        "\n",
        "# img = image.load_img(img_path, target_size=(224,224))\n",
        "# x = image.img_to_array(img)\n",
        "# x = np.expand_dims(x, axis=0)\n",
        "# x = preprocess_input(x)\n",
        "# pre = model.predict(x)\n",
        "# mmm = np.argmax(pre , axis=1)+1\n",
        "# print(mmm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzqiOrVcBcyY",
        "outputId": "c3d13c8d-fd4c-4150-ceee-c3eea5e2b4e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/init_ops.py:93: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/init_ops.py:93: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = np.asanyarray(arr)\n",
            "/usr/local/lib/python3.8/dist-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    image_name  label\n",
            "0      335.jpg      4\n",
            "1      384.jpg      0\n",
            "2      596.jpg      0\n",
            "3       15.jpg      0\n",
            "4      439.jpg      4\n",
            "..         ...    ...\n",
            "683    161.jpg      5\n",
            "684    428.jpg      1\n",
            "685    172.jpg      5\n",
            "686    279.jpg      5\n",
            "687    314.jpg      5\n",
            "\n",
            "[688 rows x 2 columns]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "s7yJ9t-Ka_iA",
        "jb1xuMTaYA__"
      ]
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}